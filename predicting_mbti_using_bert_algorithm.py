# -*- coding: utf-8 -*-
"""Predicting_MBTI_using_BERT_algorithm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IBaXwOQ2H9GQNdKxH0YzT0RZ1aYJ9isY
"""

#importing the necessary libraries
import torch
import pandas as pd
import re
import matplotlib.pyplot as plt


from torch.utils.data import TensorDataset

from sklearn.model_selection import train_test_split
from tqdm.notebook import tqdm
!pip install transformers
from transformers import BertTokenizer
from transformers import BertForSequenceClassification

#importing the dataset into a DataFrame
df = pd.read_csv('mbti_1.csv')

#showing the first 5 rows of the dataset
df.head()

#showing dataset infos
df.info()

#showing the posts of the first user in the dataset
df.posts.values[0]

#showing the number of users per personality type
df.type.value_counts()

#visualizing the number of users per personality type using a histogram
import seaborn as sns
plt.figure(figsize=(20,10))
sns.countplot(df.type)
plt.xlabel('Types count');

#removing URLs and punctuation from dataset
for index in df.index.values:
  df.posts.iloc[index] = ' '.join(df.posts.iloc[index].split('|||'))
  df.posts.iloc[index] = re.sub(r"http\S+","",df.posts.iloc[index])
  df.posts.iloc[index] = re.sub(r"[-/@.?!_,:;()|0-9]","",df.posts.iloc[index])
  df.posts.iloc[index] = ' '.join(df.posts.iloc[index].split('  '))
df.head(10)

"""
saving the clean dataset into a csv file
df.to_csv('mbti_clean_dataset.csv',index=False)
"""

#identifying the different classes of users in the dataset
labels = df.type.unique()
labels

#mapping personality types with their numberical representation
labels2 = []
label_rep = {}
for index,labels in enumerate(labels):
    label_rep[labels] = index
    labels2.append(labels)
labels2

label_rep

#replacing each personality type with its numerical representation
df['label'] = df.type.replace(label_rep)
df.head(10)

#Splitting the dataset into training (85% of the data) and test (15% of the data) sets
x_train, x_test, y_train, y_test = train_test_split(df.index.values, df.label.values, test_size=0.15, random_state=17, stratify=df.label.values)

#creating a new column "data_type"
df['data_type'] = ['not_set']*df.shape[0]

#identifying the rows belonging to both training and test sets
df.loc[x_train,'data_type'] = 'train'
df.loc[x_test,'data_type'] = 'test'

df.head(10)

#showing the amount of data reserved for training and test per personality type
df.groupby(['type','label','data_type']).count()

#creating a BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',
                                          do_lower_case=True)

#encoding the data using our tokenizer
encoded_data_train = tokenizer.batch_encode_plus(
    df[df.data_type=='train'].posts.values,
    add_special_tokens=True,
    return_attention_mask=True,
    pad_to_max_length=True,
    max_length=256,
    return_tensors='pt'
)

encoded_data_test = tokenizer.batch_encode_plus(
    df[df.data_type=='test'].posts.values,
    add_special_tokens=True,
    return_attention_mask=True,
    pad_to_max_length=True,
    max_length=256,
    return_tensors='pt'
)

#showing an example of data tokenized and encoded by BERT tokenizer
encoded_data_test

#preparing inputs for BERT model
input_ids_train = encoded_data_train['input_ids']
attention_masks_train = encoded_data_train['attention_mask']
labels_train = torch.tensor(df[df.data_type=='train'].label.values)

input_ids_test = encoded_data_test['input_ids']
attention_masks_test = encoded_data_test['attention_mask']
labels_test = torch.tensor(df[df.data_type=='test'].label.values)

dataset_train = TensorDataset(input_ids_train,attention_masks_train,labels_train)
dataset_test = TensorDataset(input_ids_test,attention_masks_test,labels_test)
len(dataset_train)

#creating a BertForSequenceClassification model
model = BertForSequenceClassification.from_pretrained("bert-base-uncased",
                                                      num_labels=len(label_rep),
                                                      output_attentions=False,
                                                      output_hidden_states=False)

from torch.utils.data import DataLoader, RandomSampler, SequentialSampler

#Defining the batch size
batch_size = 32
#less for limited hardware ressources. Example : 4

#creating dataloader to load the data while training
dataloader_train = DataLoader(dataset_train,
                              sampler=RandomSampler(dataset_train),
                              batch_size=batch_size)

dataloader_test = DataLoader(dataset_test,
                                   sampler=SequentialSampler(dataset_test),
                                   batch_size=batch_size)

from transformers import AdamW, get_linear_schedule_with_warmup

"""
Testing different learning rates for AdamW optimizer in one epoch
The following values of learning rates were tested on this code
Recommanded learning rate for BERT between 2e-5 and 5e-5
"""
learning_rate = ['2e-5','3e-5','5e-5','6e-5']
accuracy = [0.28,0.32,0.45,0.35]
plt.figure(figsize=(15,10))
plt.title('Testing different learning rates for AdamW optimizer in one epoch')
plt.ylabel('Accuracy')
plt.xlabel('Learning Rate')
plt.legend()
plt.plot(learning_rate,accuracy)

#creating an AdamW optimizer
optimizer = AdamW(model.parameters(),
                  lr=5e-5,
                  eps=1e-8)
#recommanded epsilon : 1e-8

#Defining the number of epochs
epochs = 10

#creating a scheduler to update the learning rate while training
scheduler = get_linear_schedule_with_warmup(optimizer,
                                            num_warmup_steps = 0.1,
                                            num_training_steps=len(dataloader_train)*epochs)

import numpy as np

from sklearn.metrics import f1_score

#Defining the global accuracy function
def f1_score_func(preds, labels):
    preds_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return f1_score(labels_flat, preds_flat, average='weighted')

#Defining the accuracy per class function
def accuracy_per_class(preds, labels):
    label_dict_inverse = {v: k for k, v in label_rep.items()}

    preds_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()

    for label in np.unique(labels_flat):
        y_preds = preds_flat[labels_flat==label]
        y_true = labels_flat[labels_flat==label]
        print(f'Class: {label_dict_inverse[label]}')
        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\n')
    return preds_flat, labels_flat

#Defining the device on which we're going to run the code
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
print(device)

#Defining the evaluation function to get predictions and true types
def evaluate(dataloader_test):

    model.eval()

    loss_val_total = 0
    predictions, true_vals = [], []

    for batch in dataloader_test:

        batch = tuple(b.to(device) for b in batch)

        inputs = {'input_ids':      batch[0],
                  'attention_mask': batch[1],
                  'labels':         batch[2],
                 }

        with torch.no_grad():
            outputs = model(**inputs)

        loss = outputs[0]
        logits = outputs[1]
        loss_val_total += loss.item()

        logits = logits.detach().cpu().numpy()
        label_ids = inputs['labels'].cpu().numpy()
        predictions.append(logits)
        true_vals.append(label_ids)

    loss_val_avg = loss_val_total/len(dataloader_test)

    predictions = np.concatenate(predictions, axis=0)
    true_vals = np.concatenate(true_vals, axis=0)

    return loss_val_avg, predictions, true_vals

#Training

import random
seed_value = 17
random.seed(seed_value)
np.random.seed(seed_value)
torch.manual_seed(seed_value)
torch.cuda.manual_seed_all(seed_value)
f1_score_history = []
f1_score_train_history = []
train_loss_history = []
train_test_history = []

for epoch in tqdm(range(1, epochs+1)):

    model.train()

    loss_train_total = 0

    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)
    for batch in progress_bar:

        model.zero_grad()

        batch = tuple(b.to(device) for b in batch)

        inputs = {'input_ids':      batch[0],
                  'attention_mask': batch[1],
                  'labels':         batch[2],
                 }

        outputs = model(**inputs)

        loss = outputs[0]
        loss_train_total += loss.item()
        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        optimizer.step()
        scheduler.step()

        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})


    torch.save(model.state_dict(), f'finetuned_BERT_epoch_{epoch}.model')

    tqdm.write(f'\nEpoch {epoch}')

    loss_train_avg = loss_train_total/len(dataloader_train)
    tqdm.write(f'Training loss: {loss_train_avg}')

    val_loss, predictions, true_vals = evaluate(dataloader_test)
    tr_loss, predictions_train, true_vals_train = evaluate(dataloader_train)
    val_f1 = f1_score_func(predictions, true_vals)
    tr_f1 = f1_score_func(predictions_train, true_vals_train)
    f1_score_history.append(val_f1)
    f1_score_train_history.append(tr_f1)
    tqdm.write(f'Test loss: {val_loss}')
    train_test_history.append(val_loss)
    tqdm.write(f'F1 Score test (Weighted): {val_f1}')

#Plotting the training and test accuracy per epoch

plt.figure(figsize=(15,10))
plt.plot(range(1,epochs+1),f1_score_train_history,label='Training loss')
plt.plot(range(1,epochs+1),f1_score_history,label='Test loss')
plt.title('Training history')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()

#creating a BertForSequenceClassification model
model = BertForSequenceClassification.from_pretrained("bert-base-uncased",
                                                      num_labels=len(label_rep),
                                                      output_attentions=False,
                                                      output_hidden_states=False)

model.to(device)

#Loading the model we saved in a file
model.load_state_dict(torch.load('finetuned_BERT_epoch_4.model', map_location=torch.device('cuda')))

#Getting the predictions and the types of the users in the test set
_, predictions, true_vals = evaluate(dataloader_test)

#Getting the accuracy per class of each personality type
y_preds, y_true = accuracy_per_class(predictions, true_vals)

#Plotting the confusion matrix in order to identifiy the misclassified types
from sklearn import metrics
def show_confusion_matrix(confusion_matrix):
  plt.figure(figsize=(20,10))
  hmap = sns.heatmap(confusion_matrix, annot=True, fmt="d", cmap="Blues")
  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')
  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')
  plt.ylabel('True Type')
  plt.xlabel('Predicted Type');

cm = metrics.confusion_matrix(y_true, y_preds)
df_cm = pd.DataFrame(cm, index=labels2, columns=labels2)
show_confusion_matrix(df_cm)

"""
In the next step, we're going to try to understand the reason why certain personality types are being misclassified.
We're gonna use INFP and INFJ personality types as an example.
We're going to identify the vocabulary used in commun between these two types and calculate its percentage
"""
infj = df[df.type=='INFJ'].posts.values

infp = df[df.type=='INFP'].posts.values

infj_tokens = []
for post in infj:
  tokens = tokenizer.tokenize(post)
  infj_tokens = infj_tokens + tokens
infj_tokens = list(set(infj_tokens))
len(infj_tokens)

infp_tokens = []
for post in infp:
  tokens = tokenizer.tokenize(post)
  infp_tokens = infp_tokens + tokens
infp_tokens = list(set(infp_tokens))
len(infp_tokens)

def intersection(lst1, lst2):
    lst3 = [value for value in lst1 if value in lst2]
    return lst3

inter = intersection(infj_tokens,infp_tokens)

len(inter)/len(infp_tokens)*100

len(inter)/len(infj_tokens)*100

"""The results obtained are realistic, since INFPs and INFJs share many personality traits according to psychologists."""

#Identifying the missclassifications in each personaity trait
def translate(list1 , list2):
  lst=[]
  for x in list1:
    if x in list2:
      lst.append(0)
    else:
      lst.append(1)
  return(lst)
y_true_ie=translate(y_true,[0,6,3,2,10,8,11,9])
y_preds_ie=translate(y_preds,[0,6,3,2,10,8,11,9])

y_true_sn=translate(y_true,[15,13,14,12,10,8,11,9])
y_preds_sn=translate(y_preds,[15,13,14,12,10,8,11,9])

y_true_tf=translate(y_true,[4,1,14,12,3,2,11,9])
y_preds_tf=translate(y_preds,[4,1,14,12,3,2,11,9])

y_true_jp=translate(y_true,[5,4,15,14,0,3,10,11])
y_preds_jp=translate(y_preds,[5,4,15,14,0,3,10,11])

#Plotting the confusion matrix for Introverts vs Extroverts
cm = metrics.confusion_matrix(y_true_ie, y_preds_ie)
df_cm = pd.DataFrame(cm, index=['Introvert','Extrovert'], columns=['Introvert','Extrovert'])
show_confusion_matrix(df_cm)

#Plotting the confusion matrix for Sensing vs Intuitive
cm = metrics.confusion_matrix(y_true_sn, y_preds_sn)
df_cm = pd.DataFrame(cm, index=['Sensing','Intuition'], columns=['Sensing','Intuition'])
show_confusion_matrix(df_cm)

#Plotting the confusion matrix for Thinking vs Feeling
cm = metrics.confusion_matrix(y_true_tf, y_preds_tf)
df_cm = pd.DataFrame(cm, index=['Thinking','Feeling'], columns=['Thinking','Feeling'])
show_confusion_matrix(df_cm)

#Plotting the confusion matrix for Judging vs Perceiving
cm = metrics.confusion_matrix(y_true_jp, y_preds_jp)
df_cm = pd.DataFrame(cm, index=['Judging','Perceiving'], columns=['Judging','Perceiving'])
show_confusion_matrix(df_cm)